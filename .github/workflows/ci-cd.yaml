name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_REGION: ap-south-1
  ECR_REPOSITORY: sre-demo-app
  K8S_NAMESPACE: sre-demo

jobs:
  test:
    name: Test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        # Fix: Specify the correct path to package-lock.json
        cache: 'npm'
        cache-dependency-path: './app/package-lock.json'
    
    - name: Install dependencies
      run: |
        cd app
        npm ci
      #working-directory: ./app
    
    - name: Run unit tests
      run: |
        cd app
        npm test --if-present
      #working-directory: ./app
    
    - name: Run linting
      run: |
        cd app
        # Install eslint if not present
        npm install --no-save eslint
        npx eslint src/ --ext .js --no-error-on-unmatched-pattern || true
      #working-directory: ./app
    
    - name: Run security audit
      run: |
        cd app
        npm audit --production || true
      #working-directory: ./app

  build-and-push:
    name: Build and Push Docker Image
    needs: test
    runs-on: ubuntu-latest
    # Only run on main branch pushes
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1
    
    - name: Build, tag, and push image to Amazon ECR
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        # Check if repository exists, create if it doesn't
        aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} || \
        aws ecr create-repository --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }}
        
        cd app
        docker build -t $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:$IMAGE_TAG .
        docker build -t $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:latest .
        docker push $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:$IMAGE_TAG
        docker push $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:latest
        echo "IMAGE_URI=$ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:$IMAGE_TAG" >> $GITHUB_ENV

  deploy-canary:
    name: Deploy Canary
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://sre-demo.example.com
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Update kubeconfig
      run: |
        # Install kubectl if not present
        curl -LO "https://dl.k8s.io/release/v1.27.0/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        
        # Configure kubectl
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name sre-demo-dev || \
        echo "EKS cluster not found, skipping kubectl config"
    
    - name: Create canary deployment
      run: |
        # Create canary namespace if it doesn't exist
        kubectl create namespace ${{ env.K8S_NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
        
        # Create canary deployment with 10% traffic
        cat <<EOF | kubectl apply -f -
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: sre-demo-app-canary
          namespace: ${{ env.K8S_NAMESPACE }}
          labels:
            app: sre-demo
            component: api-canary
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: sre-demo
              component: api-canary
          template:
            metadata:
              labels:
                app: sre-demo
                component: api-canary
              annotations:
                prometheus.io/scrape: "true"
                prometheus.io/port: "3000"
                prometheus.io/path: "/metrics"
            spec:
              containers:
              - name: app
                image: ${{ env.IMAGE_URI }}
                ports:
                - containerPort: 3000
                env:
                - name: NODE_ENV
                  value: "production"
                - name: CANARY
                  value: "true"
                livenessProbe:
                  httpGet:
                    path: /health
                    port: 3000
                  initialDelaySeconds: 30
                  periodSeconds: 10
                readinessProbe:
                  httpGet:
                    path: /ready
                    port: 3000
                  initialDelaySeconds: 5
                  periodSeconds: 5
        EOF
        
        # Create canary service
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Service
        metadata:
          name: sre-demo-service-canary
          namespace: ${{ env.K8S_NAMESPACE }}
          labels:
            app: sre-demo
            component: api-canary
        spec:
          selector:
            app: sre-demo
            component: api-canary
          ports:
          - port: 80
            targetPort: 3000
          type: ClusterIP
        EOF
    
    - name: Monitor canary health
      run: |
        # Wait for canary to be ready
        echo "Waiting for canary deployment to be ready..."
        kubectl rollout status deployment/sre-demo-app-canary -n ${{ env.K8S_NAMESPACE }} --timeout=120s || exit 1
        
        # Test canary health endpoint
        echo "Testing canary health endpoint..."
        kubectl run test-connection --image=curlimages/curl:latest --restart=Never --rm -it -- \
          curl -s -f http://sre-demo-service-canary.${{ env.K8S_NAMESPACE }}.svc.cluster.local/health || exit 1
        
        # Check for errors in logs
        echo "Checking canary logs for errors..."
        ERROR_COUNT=$(kubectl logs -l app=sre-demo,component=api-canary -n ${{ env.K8S_NAMESPACE }} --tail=100 2>/dev/null | grep -c "error" || echo "0")
        if [ "$ERROR_COUNT" -gt "0" ]; then
          echo "Found $ERROR_COUNT errors in canary logs"
          exit 1
        fi
        
        echo "âœ… Canary health check passed"
    
    - name: Promote to stable
      if: success()
      run: |
        echo "Promoting canary to stable..."
        
        # Get the main deployment if it exists
        if kubectl get deployment sre-demo-app -n ${{ env.K8S_NAMESPACE }} &>/dev/null; then
          # Update existing deployment
          kubectl set image deployment/sre-demo-app -n ${{ env.K8S_NAMESPACE }} app=${{ env.IMAGE_URI }}
          kubectl rollout status deployment/sre-demo-app -n ${{ env.K8S_NAMESPACE }} --timeout=180s
        else
          # Create main deployment from canary
          cat <<EOF | kubectl apply -f -
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: sre-demo-app
            namespace: ${{ env.K8S_NAMESPACE }}
            labels:
              app: sre-demo
              component: api
          spec:
            replicas: 3
            selector:
              matchLabels:
                app: sre-demo
                component: api
            template:
              metadata:
                labels:
                  app: sre-demo
                  component: api
                annotations:
                  prometheus.io/scrape: "true"
                  prometheus.io/port: "3000"
                  prometheus.io/path: "/metrics"
              spec:
                containers:
                - name: app
                  image: ${{ env.IMAGE_URI }}
                  ports:
                  - containerPort: 3000
                  env:
                  - name: NODE_ENV
                    value: "production"
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 3000
                    initialDelaySeconds: 30
                    periodSeconds: 10
                  readinessProbe:
                    httpGet:
                      path: /ready
                      port: 3000
                    initialDelaySeconds: 5
                    periodSeconds: 5
                  resources:
                    requests:
                      memory: "128Mi"
                      cpu: "100m"
                    limits:
                      memory: "256Mi"
                      cpu: "500m"
          EOF
          
          # Create service if it doesn't exist
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Service
          metadata:
            name: sre-demo-service
            namespace: ${{ env.K8S_NAMESPACE }}
            labels:
              app: sre-demo
              component: api
          spec:
            selector:
              app: sre-demo
              component: api
            ports:
            - port: 80
              targetPort: 3000
            type: ClusterIP
          EOF
        fi
        
        # Delete canary resources
        kubectl delete deployment sre-demo-app-canary -n ${{ env.K8S_NAMESPACE }} --ignore-not-found
        kubectl delete service sre-demo-service-canary -n ${{ env.K8S_NAMESPACE }} --ignore-not-found
        
        echo "âœ… Successfully promoted to stable"
    
    - name: Rollback on failure
      if: failure()
      run: |
        echo "âŒ Canary deployment failed, rolling back..."
        
        # Delete canary resources
        kubectl delete deployment sre-demo-app-canary -n ${{ env.K8S_NAMESPACE }} --ignore-not-found
        kubectl delete service sre-demo-service-canary -n ${{ env.K8S_NAMESPACE }} --ignore-not-found
        
        # Alert to Slack (if webhook configured)
        if [ -n "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
          curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"ðŸš¨ Canary deployment failed for commit ${{ github.sha }}. Rolling back.\"}" \
            ${{ secrets.SLACK_WEBHOOK_URL }}
        fi
        
        echo "Rollback completed"

  security-scan:
    name: Security Scan
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }}
        format: 'sarif'
        output: 'trivy-results.sarif'
        severity: 'CRITICAL,HIGH'
        ignore-unfixed: true
        exit-code: '0'  # Don't fail the build on vulnerabilities
    
    - name: Upload Trivy results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'Â Ì‘S
